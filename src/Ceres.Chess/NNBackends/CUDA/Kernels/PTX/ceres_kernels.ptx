//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29069683
// Cuda compilation tools, release 11.1, V11.1.74
// Based on LLVM 3.4svn
//

.version 7.1
.target sm_60
.address_size 64

	// .globl	_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi

.visible .entry _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi(
	.param .u64 _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_0,
	.param .u64 _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_1,
	.param .u64 _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_2,
	.param .u32 _ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_3
)
{
	.reg .pred 	%p<3>;
	.reg .b16 	%rs<9>;
	.reg .f32 	%f<9>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<35>;


	ld.param.u64 	%rd10, [_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_0];
	ld.param.u64 	%rd11, [_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_1];
	ld.param.u64 	%rd12, [_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_2];
	ld.param.u32 	%r8, [_ZN5ceres21copyMaskedMovesKernelEP6__halfPsPfi_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	setp.ge.s32	%p1, %r4, %r8;
	@%p1 bra 	BB0_3;

	cvta.to.global.u64 	%rd1, %rd10;
	mul.lo.s32 	%r5, %r4, 1858;
	mul.lo.s32 	%r10, %r1, %r2;
	mul.lo.s32 	%r11, %r10, 96;
	mad.lo.s32 	%r12, %r3, 96, %r11;
	cvt.s64.s32	%rd13, %r12;
	neg.s64 	%rd33, %rd13;
	mul.wide.s32 	%rd3, %r12, 2;
	cvta.to.global.u64 	%rd34, %rd11;
	cvta.to.global.u64 	%rd5, %rd12;
	mov.u32 	%r29, -96;

BB0_2:
	add.s64 	%rd14, %rd34, %rd3;
	ld.global.s16 	%r13, [%rd14];
	add.s32 	%r14, %r13, %r5;
	mul.wide.s32 	%rd15, %r14, 2;
	add.s64 	%rd16, %rd1, %rd15;
	ld.global.u16 	%rs1, [%rd16];
	// inline asm
	{  cvt.f32.f16 %f1, %rs1;}

	// inline asm
	shl.b64 	%rd17, %rd33, 2;
	sub.s64 	%rd18, %rd5, %rd17;
	st.global.f32 	[%rd18], %f1;
	ld.global.s16 	%r15, [%rd14+2];
	add.s32 	%r16, %r15, %r5;
	mul.wide.s32 	%rd19, %r16, 2;
	add.s64 	%rd20, %rd1, %rd19;
	ld.global.u16 	%rs2, [%rd20];
	// inline asm
	{  cvt.f32.f16 %f2, %rs2;}

	// inline asm
	st.global.f32 	[%rd18+4], %f2;
	ld.global.s16 	%r17, [%rd14+4];
	add.s32 	%r18, %r17, %r5;
	mul.wide.s32 	%rd21, %r18, 2;
	add.s64 	%rd22, %rd1, %rd21;
	ld.global.u16 	%rs3, [%rd22];
	// inline asm
	{  cvt.f32.f16 %f3, %rs3;}

	// inline asm
	st.global.f32 	[%rd18+8], %f3;
	ld.global.s16 	%r19, [%rd14+6];
	add.s32 	%r20, %r19, %r5;
	mul.wide.s32 	%rd23, %r20, 2;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u16 	%rs4, [%rd24];
	// inline asm
	{  cvt.f32.f16 %f4, %rs4;}

	// inline asm
	st.global.f32 	[%rd18+12], %f4;
	ld.global.s16 	%r21, [%rd14+8];
	add.s32 	%r22, %r21, %r5;
	mul.wide.s32 	%rd25, %r22, 2;
	add.s64 	%rd26, %rd1, %rd25;
	ld.global.u16 	%rs5, [%rd26];
	// inline asm
	{  cvt.f32.f16 %f5, %rs5;}

	// inline asm
	st.global.f32 	[%rd18+16], %f5;
	ld.global.s16 	%r23, [%rd14+10];
	add.s32 	%r24, %r23, %r5;
	mul.wide.s32 	%rd27, %r24, 2;
	add.s64 	%rd28, %rd1, %rd27;
	ld.global.u16 	%rs6, [%rd28];
	// inline asm
	{  cvt.f32.f16 %f6, %rs6;}

	// inline asm
	st.global.f32 	[%rd18+20], %f6;
	ld.global.s16 	%r25, [%rd14+12];
	add.s32 	%r26, %r25, %r5;
	mul.wide.s32 	%rd29, %r26, 2;
	add.s64 	%rd30, %rd1, %rd29;
	ld.global.u16 	%rs7, [%rd30];
	// inline asm
	{  cvt.f32.f16 %f7, %rs7;}

	// inline asm
	st.global.f32 	[%rd18+24], %f7;
	ld.global.s16 	%r27, [%rd14+14];
	add.s32 	%r28, %r27, %r5;
	mul.wide.s32 	%rd31, %r28, 2;
	add.s64 	%rd32, %rd1, %rd31;
	ld.global.u16 	%rs8, [%rd32];
	// inline asm
	{  cvt.f32.f16 %f8, %rs8;}

	// inline asm
	st.global.f32 	[%rd18+28], %f8;
	add.s64 	%rd34, %rd34, 16;
	add.s64 	%rd33, %rd33, -8;
	add.s32 	%r29, %r29, 8;
	setp.ne.s32	%p2, %r29, 0;
	@%p2 bra 	BB0_2;

BB0_3:
	ret;
}

	// .globl	_ZN5ceres18shiftConvertKernelEP6__halfPcffi
.visible .entry _ZN5ceres18shiftConvertKernelEP6__halfPcffi(
	.param .u64 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_0,
	.param .u64 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_1,
	.param .f32 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_2,
	.param .f32 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_3,
	.param .u32 _ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<3>;
	.reg .f32 	%f<8>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_0];
	ld.param.u64 	%rd2, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_1];
	ld.param.f32 	%f1, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_2];
	ld.param.f32 	%f2, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_3];
	ld.param.u32 	%r2, [_ZN5ceres18shiftConvertKernelEP6__halfPcffi_param_4];
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %ntid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r3, %r4, %r5;
	setp.ge.s32	%p1, %r1, %r2;
	@%p1 bra 	BB1_2;

	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	shl.b32 	%r6, %r1, 1;
	cvt.s64.s32	%rd5, %r6;
	add.s64 	%rd6, %rd3, %rd5;
	ld.global.s8 	%rs2, [%rd6+1];
	mul.wide.s16 	%r7, %rs2, 256;
	ld.global.s8 	%r8, [%rd6];
	add.s32 	%r9, %r7, %r8;
	cvt.rn.f32.s32	%f4, %r9;
	sub.f32 	%f5, %f2, %f1;
	mul.f32 	%f6, %f5, %f4;
	div.rn.f32 	%f7, %f6, 0f477FFF00;
	add.f32 	%f3, %f7, %f1;
	// inline asm
	{  cvt.rn.f16.f32 %rs1, %f3;}

	// inline asm
	mul.wide.s32 	%rd7, %r1, 2;
	add.s64 	%rd8, %rd4, %rd7;
	st.global.u16 	[%rd8], %rs1;

BB1_2:
	ret;
}


